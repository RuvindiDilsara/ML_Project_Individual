# -*- coding: utf-8 -*-
"""Layer_10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zm-Kqu7QwMKC-2QFGaa-erClnbWZyduc
"""

!pip install shap

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier
import pickle

"""Load the data and preprocess"""

from google.colab import drive
MOUNT_PATH='/content/drive'
drive.mount(MOUNT_PATH)

WORKING_DIR=f"{MOUNT_PATH}/MyDrive/ML/ML_Project_Individual/Layer_10"
DATASET_DIR=f"{MOUNT_PATH}/MyDrive/ML/ML_Group_Project"

train = pd.read_csv(f"{DATASET_DIR}/layer_10_train.csv")
train.head()

valid = pd.read_csv(f"{DATASET_DIR}/layer_10_valid.csv")
valid.head()

test = pd.read_csv(f"{DATASET_DIR}/layer_10_test.csv")
test.head()

#constants
L1 = 'label_1'
L2 = 'label_2'
L3 = 'label_3'
L4 = 'label_4'

LABELS = [L1, L2, L3, L4]
SPEAKER_LABEL = L1
AGE_LABEL = L2
GENDER_LABEL = L3
ACCENT_LABEL = L4
FEATURES = [f"feature_{i}" for i in range (1,769)]

x_train = {}
y_train = {}
x_valid = {}
y_valid = {}
x_test = {}
y_test = {}

for target_label in LABELS:
  tr_df = train[train['label_2'].notna()] if target_label == 'label_2' else train
  vl_df = valid[valid['label_2'].notna()] if target_label == 'label_2' else valid
  test_df = test

  scaler = StandardScaler()
  x_train[target_label] = pd.DataFrame(scaler.fit_transform(tr_df.drop(LABELS, axis = 1)), columns=FEATURES)
  y_train[target_label] = tr_df[target_label]

  x_valid[target_label] = pd.DataFrame(scaler.transform(vl_df.drop(LABELS, axis = 1)), columns=FEATURES)
  y_valid[target_label] = vl_df[target_label]

  x_test[target_label] = pd.DataFrame(scaler.transform(vl_df.drop(LABELS, axis = 1)), columns=FEATURES)
  # y_test[target_label] = test_df[target_label]

import shap
import matplotlib.pyplot as plt
shap.initjs()
def getShapValues(model, X_valid):
    # Fits the explainer
    explainer = shap.Explainer(model.predict, X_valid)
    # Calculates the SHAP values - It takes some time
    shap_values = explainer(X_valid, max_evals=2 * X_valid.shape[1]+1)
    shap.plots.bar(shap_values)

"""# Label 1 - Speaker"""

clf = SVC(kernel = 'linear')
clf.fit(x_train[L1], y_train[L1])

y_pred = clf.predict(x_valid[L1])
y_pred_test = clf.predict(x_test[L1])

print (metrics.confusion_matrix(y_valid[L1], y_pred))
print (metrics.accuracy_score(y_valid[L1], y_pred))
print (metrics.precision_score(y_valid[L1], y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L1], y_pred, average="weighted"))

"""Feature Engineering"""

pca = PCA(n_components=0.95, svd_solver='full')
pca.fit(x_train[L1])
x_train_trf = pd.DataFrame(pca.transform(x_train[L1]))
x_valid_trf = pd.DataFrame(pca.transform(x_valid[L1]))
x_test_trf = pd.DataFrame(pca.transform(x_test[L1]))
print("Shape after PCA: ", x_train_trf.shape)

"""After feature engineering"""

clf = SVC(kernel = 'linear')
clf.fit(x_train_trf, y_train[L1])

y_pred = clf.predict(x_valid_trf)
print (metrics.confusion_matrix(y_valid[L1], y_pred))
print (metrics.accuracy_score(y_valid[L1], y_pred))
print (metrics.precision_score(y_valid[L1], y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L1], y_pred, average="weighted"))

"""# Hyperparameter Tuning and Cross Validation

## Try for SVM
"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

param_dist = {
    'C': np.logspace(-3, 3, 6),  # Example range for C from 0.001 to 1000
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': np.logspace(-3, 3, 6)  # Example range for gamma from 0.001 to 1000
}


svm = SVC()

random_search = RandomizedSearchCV(
    estimator=svm, param_distributions=param_dist, scoring='accuracy', cv=5, verbose=1, n_jobs=-1, n_iter=4, random_state=42
)

#add random_state=42 to stay with the same random value

random_search.fit(x_train_trf, y_train[L1])

best_params = random_search.best_params_
best_model = random_search.best_estimator_

y_pred = best_model.predict(x_valid_trf)
accuracy = accuracy_score(y_valid[L1], y_pred)
print(f'Accuracy on test data: {accuracy}')

"""Evaluate the accuracy"""

print (metrics.confusion_matrix(y_valid[L1], y_pred))
print (metrics.accuracy_score(y_valid[L1], y_pred))
print (metrics.precision_score(y_valid[L1], y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L1], y_pred, average="weighted"))

print("best_params:", best_params)

print('best model:\n', best_model)

"""new"""

best_clf_L1 = SVC(C=63.0957344480193, gamma=0.015848931924611134, kernel='poly')
best_clf_L1.fit(x_train_trf, y_train[L1])

from sklearn.metrics import accuracy_score
y_pred_L1 = best_clf_L1.predict(x_valid_trf)
accuracy = accuracy_score(y_valid[L1], y_pred_L1)
print(f'Accuracy on test data: {accuracy}')

# getShapValues(best_clf_L1,x_valid_trf)

print (metrics.confusion_matrix(y_valid[L1], y_pred_L1))
print (metrics.accuracy_score(y_valid[L1], y_pred_L1))
print (metrics.precision_score(y_valid[L1], y_pred_L1, average="weighted"))
print (metrics.recall_score(y_valid[L1], y_pred_L1, average="weighted"))

import joblib

# Save the model to your Google Drive using joblib
joblib.dump(best_clf_L1, f'{WORKING_DIR}/best_model_L1_new.joblib')

loaded_model = joblib.load(f'{WORKING_DIR}/best_model_L1_new.joblib')

y_pred_L1_load = loaded_model.predict(x_valid_trf)
accuracy = accuracy_score(y_valid[L1], y_pred_L1_load)
print(f'Accuracy on test data: {accuracy}')

# label_1_x_test_trf = pd.DataFrame(pca.transform(x_test[L1]))
# Assuming label_2_x_test_trf is your test data that you want to make predictions on
label_1_y_test_pred_1 = best_model.predict(x_test_trf)

# Create a DataFrame with the test data and predictions
combined_df = pd.DataFrame({
    'ID': test_df['ID'],
    'Predicted_Label': label_1_y_test_pred_1})

# combined_df = pd.concat([test_df, pd.DataFrame({'Predicted_Label': label_1_y_test_pred_1})], axis=0)
# Save the DataFrame to a CSV file
# combined_df.to_csv('layer_9_label_2.csv', index=False)

# Save the DataFrame to the specified CSV file path
combined_df.to_csv(f"{WORKING_DIR}/layer_10_label_1.csv", index=False)

"""### Different approach"""

new_param_dist = {
    'C': np.logspace(-3, 3, 4),  # Example range for C from 0.001 to 1000
    'kernel': ['linear', 'rbf', 'poly'],
    # 'gamma': np.logspace(-3, 3, 6)  # Example range for gamma from 0.001 to 1000
    'gamma': ['scale', 'auto']
}


svm2 = SVC()

random_search_2 = RandomizedSearchCV(
    estimator=svm2, param_distributions=new_param_dist, scoring='accuracy', cv=5, verbose=2, n_jobs=-1, n_iter=5, random_state=42
)

random_search_2.fit(x_train_trf, y_train[L1])

best_params_2 = random_search_2.best_params_
best_model_2 = random_search_2.best_estimator_

y_pred_2 = best_model_2.predict(x_valid_trf)
accuracy = accuracy_score(y_valid[L1], y_pred_2)
print(f'Accuracy on test data: {accuracy}')
print("Best params: ", best_params_2)
print("Best_model: \n", best_model_2)

print (metrics.confusion_matrix(y_valid[L1], y_pred_2))
print (metrics.accuracy_score(y_valid[L1], y_pred_2))
print (metrics.precision_score(y_valid[L1], y_pred_2, average="weighted"))
print (metrics.recall_score(y_valid[L1], y_pred_2, average="weighted"))

label_1_x_test_trf = pd.DataFrame(pca.transform(x_test[L1]))
# Assuming label_2_x_test_trf is your test data that you want to make predictions on
label_1_y_test_pred_2 = best_model_2.predict(label_1_x_test_trf)

# Create a DataFrame with the test data and predictions
combined_df = pd.DataFrame({
    'ID': test_df['ID'],
    'Predicted_Label': label_1_y_test_pred_2})

# combined_df = pd.concat([test_df, pd.DataFrame({'Predicted_Label': label_1_y_test_pred_2})], axis=0)
# Save the DataFrame to a CSV file
# combined_df.to_csv('layer_9_label_2.csv', index=False)

# Save the DataFrame to the specified CSV file path
combined_df.to_csv(f"{WORKING_DIR}/layer_10_label_1_2.csv", index=False)

"""# Label 2"""

label_2_clf = SVC(kernel = 'linear', class_weight='balanced')
label_2_clf.fit(x_train[L2], y_train[L2])

label_2_y_pred = label_2_clf.predict(x_valid[L2])
label_2_y_pred_test = label_2_clf.predict(x_test[L2])

print (metrics.confusion_matrix(y_valid[L2], label_2_y_pred))
print (metrics.accuracy_score(y_valid[L2], label_2_y_pred))
print (metrics.precision_score(y_valid[L2], label_2_y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L2], label_2_y_pred, average="weighted"))

"""Feature Engineering"""

label_2_pca = PCA(n_components=0.95, svd_solver='full')
label_2_pca.fit(x_train[L2])
label_2_x_train_trf = pd.DataFrame(label_2_pca.transform(x_train[L2]))
label_2_x_valid_trf = pd.DataFrame(label_2_pca.transform(x_valid[L2]))

print("Shape after PCA: ", label_2_x_train_trf.shape)

"""After feature engineering"""

label_2_trf_clf = SVC(kernel = 'linear', class_weight='balanced')
label_2_trf_clf.fit(label_2_x_train_trf, y_train[L2])

label_2_trf_y_pred = label_2_trf_clf.predict(label_2_x_valid_trf)
print (metrics.confusion_matrix(y_valid[L2], label_2_trf_y_pred))
print (metrics.accuracy_score(y_valid[L2], label_2_trf_y_pred))
print (metrics.precision_score(y_valid[L2], label_2_trf_y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L2], label_2_trf_y_pred, average="weighted"))

"""# Hyperparameter Tuning and Cross Validation

Using SVM
"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

param_dist = {
    'C': np.logspace(-3, 3, 6),  # Example range for C from 0.001 to 1000
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': np.logspace(-3, 3, 6)  # Example range for gamma from 0.001 to 1000
}


label_2_svm = SVC()

label_2_random_search = RandomizedSearchCV(
    estimator=label_2_svm, param_distributions=param_dist, scoring='accuracy', cv=5, verbose=1, n_jobs=-1, n_iter=4, random_state=42
)

label_2_random_search.fit(label_2_x_train_trf, y_train[L2])

label_2_best_params = label_2_random_search.best_params_
label_2_best_model = label_2_random_search.best_estimator_

label_2_y_pred = label_2_best_model.predict(label_2_x_valid_trf)
accuracy = accuracy_score(y_valid[L2], label_2_y_pred)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L2], label_2_y_pred))
print (metrics.accuracy_score(y_valid[L2], label_2_y_pred))
print (metrics.precision_score(y_valid[L2], label_2_y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L2], label_2_y_pred, average="weighted"))

print("best_params:", label_2_best_params)
print('best model:\n', label_2_best_model)

"""transform test dataset"""

label_2_x_test_trf = pd.DataFrame(label_2_pca.transform(x_test[L2]))

# Assuming label_2_x_test_trf is your test data that you want to make predictions on
label_2_y_test_pred = label_2_best_model.predict(label_2_x_test_trf)

# Create a DataFrame with the test data and predictions
combined_df = pd.DataFrame({
    'ID': test_df['ID'],
    'label_2': label_2_y_test_pred})

# combined_df = pd.concat([test_df, pd.DataFrame({'Predicted_Label': label_2_y_test_pred})], axis=0)
# Save the DataFrame to a CSV file
# combined_df.to_csv('layer_9_label_2.csv', index=False)

# Save the DataFrame to the specified CSV file path
combined_df.to_csv(f"{WORKING_DIR}/layer_10_label_2.csv", index=False)

best_clf_L2 = SVC(C=63.0957344480193, gamma=0.015848931924611134, kernel='poly')
best_clf_L2.fit(label_2_x_train_trf, y_train[L2])

y_pred_L2 = best_clf_L2.predict(label_2_x_valid_trf)
accuracy = accuracy_score(y_valid[L2], y_pred_L2)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L2], y_pred_L2))
print (metrics.accuracy_score(y_valid[L2], y_pred_L2))
print (metrics.precision_score(y_valid[L2], y_pred_L2, average="weighted"))
print (metrics.recall_score(y_valid[L2], y_pred_L2, average="weighted"))

# Save the model to your Google Drive using joblib
joblib.dump(best_clf_L2, f'{WORKING_DIR}/best_model_L2.joblib')

loaded_model = joblib.load(f'{WORKING_DIR}/best_model_L2.joblib')

y_pred_L2_load = loaded_model.predict(label_2_x_valid_trf)
accuracy = accuracy_score(y_valid[L2], y_pred_L2_load)
print(f'Accuracy on test data: {accuracy}')

"""# Label 3"""

label_3_clf = SVC(kernel = 'linear', class_weight='balanced')
label_3_clf.fit(x_train[L3], y_train[L3])

label_3_y_pred = label_3_clf.predict(x_valid[L3])
label_3_y_pred_test = label_3_clf.predict(x_test[L3])

print (metrics.confusion_matrix(y_valid[L3], label_3_y_pred))
print (metrics.accuracy_score(y_valid[L3], label_3_y_pred))
print (metrics.precision_score(y_valid[L3], label_3_y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L3], label_3_y_pred, average="weighted"))

"""## Feature Engineering"""

label_3_pca = PCA(n_components=0.95, svd_solver='full')
label_3_pca.fit(x_train[L3])
label_3_x_train_trf = pd.DataFrame(label_3_pca.transform(x_train[L3]))
label_3_x_valid_trf = pd.DataFrame(label_3_pca.transform(x_valid[L3]))
label_3_x_test_trf = pd.DataFrame(label_3_pca.transform(x_test[L3]))

print("Shape after PCA: ", label_3_x_train_trf.shape)

"""After feature engineering"""

label_3_trf_clf = SVC(kernel = 'linear', class_weight='balanced')
label_3_trf_clf.fit(label_3_x_train_trf, y_train[L3])

label_3_trf_y_pred = label_3_trf_clf.predict(label_3_x_valid_trf)
print (metrics.confusion_matrix(y_valid[L3], label_3_trf_y_pred))
print (metrics.accuracy_score(y_valid[L3], label_3_trf_y_pred))
print (metrics.precision_score(y_valid[L3], label_3_trf_y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L3], label_3_trf_y_pred, average="weighted"))

"""## Hyperparameter Tuning and Cross Validation"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

param_dist = {
    'C': np.logspace(-3, 3, 6),  # Example range for C from 0.001 to 1000
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': np.logspace(-3, 3, 6)  # Example range for gamma from 0.001 to 1000
}


label_3_svm = SVC()

label_3_random_search = RandomizedSearchCV(
    estimator=label_3_svm, param_distributions=param_dist, scoring='accuracy', cv=5, verbose=1, n_jobs=-1, n_iter=2, random_state=42
)

#add random_state=42 to stay with the same random value

label_3_random_search.fit(label_3_x_train_trf, y_train[L3])

label_3_best_params = label_3_random_search.best_params_
label_3_best_model = label_3_random_search.best_estimator_

label_3_y_pred = label_3_best_model.predict(label_3_x_valid_trf)
accuracy = accuracy_score(y_valid[L3], label_3_y_pred)
print(f'Accuracy on test data: {accuracy}')

print("best_params:", label_3_best_params)
print('best model:\n', label_3_best_model)

print (metrics.confusion_matrix(y_valid[L3], label_3_y_pred))
print (metrics.accuracy_score(y_valid[L3], label_3_y_pred))
print (metrics.precision_score(y_valid[L3], label_3_y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L3], label_3_y_pred, average="weighted"))

best_clf_L3 = SVC(C=63.0957344480193, gamma=0.015848931924611134, kernel='poly')
best_clf_L3.fit(label_3_x_train_trf, y_train[L3])

y_pred_L3 = best_clf_L3.predict(label_3_x_valid_trf)
accuracy = accuracy_score(y_valid[L3], y_pred_L3)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L3], y_pred_L3))
print (metrics.accuracy_score(y_valid[L3], y_pred_L3))
print (metrics.precision_score(y_valid[L3], y_pred_L3, average="weighted"))
print (metrics.recall_score(y_valid[L3], y_pred_L3, average="weighted"))

# Save the model to your Google Drive using joblib
joblib.dump(best_clf_L3, f'{WORKING_DIR}/best_model_L3.joblib')

loaded_model = joblib.load(f'{WORKING_DIR}/best_model_L3.joblib')

y_pred_L3_load = loaded_model.predict(label_3_x_valid_trf)
accuracy = accuracy_score(y_valid[L3], y_pred_L3_load)
print(f'Accuracy on test data: {accuracy}')

# Assuming label_2_x_test_trf is your test data that you want to make predictions on
label_3_y_test_pred = label_3_best_model.predict(label_3_x_test_trf)

# Create a DataFrame with the test data and predictions
combined_df = pd.DataFrame({
    'ID': test_df['ID'],
    'label_3': label_3_y_test_pred})

# combined_df = pd.concat([test_df, pd.DataFrame({'Predicted_Label': label_3_y_test_pred})], axis=0)
# Save the DataFrame to a CSV file
# combined_df.to_csv('layer_9_label_2.csv', index=False)

# Save the DataFrame to the specified CSV file path
combined_df.to_csv(f"{WORKING_DIR}/layer_10_label_3.csv", index=False)

"""# Label 4"""

label_4_clf = SVC(kernel = 'linear', class_weight='balanced')
label_4_clf.fit(x_train[L4], y_train[L4])

label_4_y_pred = label_4_clf.predict(x_valid[L4])
label_4_y_pred_test = label_4_clf.predict(x_test[L4])

print (metrics.confusion_matrix(y_valid[L4], label_4_y_pred))
print (metrics.accuracy_score(y_valid[L4], label_4_y_pred))
print (metrics.precision_score(y_valid[L4], label_4_y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L4], label_4_y_pred, average="weighted"))

"""## Feature Engineering"""

label_4_pca = PCA(n_components=0.95, svd_solver='full')
label_4_pca.fit(x_train[L4])
label_4_x_train_trf = pd.DataFrame(label_4_pca.transform(x_train[L4]))
label_4_x_valid_trf = pd.DataFrame(label_4_pca.transform(x_valid[L4]))
label_4_x_test_trf = pd.DataFrame(label_4_pca.transform(x_test[L4]))

print("Shape after PCA: ", label_4_x_train_trf.shape)

label_4_trf_clf = SVC(kernel = 'linear', class_weight='balanced')
label_4_trf_clf.fit(label_4_x_train_trf, y_train[L4])

label_4_trf_y_pred = label_4_trf_clf.predict(label_4_x_valid_trf)
print (metrics.confusion_matrix(y_valid[L4], label_4_trf_y_pred))
print (metrics.accuracy_score(y_valid[L4], label_4_trf_y_pred))
print (metrics.precision_score(y_valid[L4], label_4_trf_y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L4], label_4_trf_y_pred, average="weighted"))

"""# Hyperparameter Tuning and Cross Validation"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

param_dist = {
    'C': np.logspace(-3, 3, 6),  # Example range for C from 0.001 to 1000
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': np.logspace(-3, 3, 6),  # Example range for gamma from 0.001 to 1000
    'class_weight': ['balanced']
}


label_4_svm = SVC()

label_4_random_search = RandomizedSearchCV(
    estimator=label_4_svm, param_distributions=param_dist, scoring='accuracy', cv=5, verbose=2, n_jobs=-1, n_iter=2, random_state=42
)

#add random_state=42 to stay with the same random value

label_4_random_search.fit(label_4_x_train_trf, y_train[L4])

label_4_best_params = label_4_random_search.best_params_
label_4_best_model = label_4_random_search.best_estimator_

label_4_y_pred = label_4_best_model.predict(label_4_x_valid_trf)
accuracy = accuracy_score(y_valid[L4], label_4_y_pred)
print(f'Accuracy on test data: {accuracy}')

print("best_params:", label_4_best_params)
print('best model:\n', label_4_best_model)

print (metrics.confusion_matrix(y_valid[L4], label_4_y_pred))
print (metrics.accuracy_score(y_valid[L4], label_4_y_pred))
print (metrics.precision_score(y_valid[L4], label_4_y_pred, average="weighted"))
print (metrics.recall_score(y_valid[L4], label_4_y_pred, average="weighted"))

best_clf_L4 = SVC(C=63.0957344480193, gamma=0.015848931924611134, kernel='poly')
best_clf_L4.fit(label_4_x_train_trf, y_train[L4])

y_pred_L4 = best_clf_L4.predict(label_4_x_valid_trf)
accuracy = accuracy_score(y_valid[L4], y_pred_L4)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L4], y_pred_L4))
print (metrics.accuracy_score(y_valid[L4], y_pred_L4))
print (metrics.precision_score(y_valid[L4], y_pred_L4, average="weighted"))
print (metrics.recall_score(y_valid[L4], y_pred_L4, average="weighted"))

# Save the model to your Google Drive using joblib
joblib.dump(best_clf_L4, f'{WORKING_DIR}/best_model_L4.joblib')

loaded_model = joblib.load(f'{WORKING_DIR}/best_model_L4.joblib')

y_pred_L4_load = loaded_model.predict(label_4_x_valid_trf)
accuracy = accuracy_score(y_valid[L4], y_pred_L4_load)
print(f'Accuracy on test data: {accuracy}')

# Assuming label_2_x_test_trf is your test data that you want to make predictions on
label_4_y_test_pred = label_4_best_model.predict(label_4_x_test_trf)

# Create a DataFrame with the test data and predictions
combined_df = pd.DataFrame({
    'ID': test_df['ID'],
    'label_4': label_4_y_test_pred})

# combined_df = pd.concat([test_df, pd.DataFrame({'Predicted_Label': label_4_y_test_pred})], axis=0)
# Save the DataFrame to a CSV file
# combined_df.to_csv('layer_9_label_2.csv', index=False)

# Save the DataFrame to the specified CSV file path
combined_df.to_csv(f"{WORKING_DIR}/layer_10_label_4.csv", index=False)

"""# Finalizing the CSV"""

output_df = pd.DataFrame({
    'ID': test_df['ID'],
    'label_1': label_1_y_test_pred_1,
    'label_2': label_2_y_test_pred,
    'label_3': label_3_y_test_pred,
    'label_4': label_4_y_test_pred
})

output_df.shape
output_df.head()

# Save the DataFrame to the specified CSV file path
output_df.to_csv(f"{WORKING_DIR}/190140L_layer_10.csv", index=False)

predict_df = pd.DataFrame({
    'label_1': y_pred_L1,
    'label_3': y_pred_L3,
    'label_4': y_pred_L4,
})

predict_df.shape
predict_df.head()

# Save the DataFrame to the specified CSV file path
predict_df.to_csv(f"{WORKING_DIR}/prediction_layer_10.csv", index=False)

predict_df_L2 = pd.DataFrame({
    'label_2': y_pred_L2
})

predict_df_L2.shape
predict_df_L2.head()

# Save the DataFrame to the specified CSV file path
predict_df_L2.to_csv(f"{WORKING_DIR}/prediction_layer_10_L2.csv", index=False)